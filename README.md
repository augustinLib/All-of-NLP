# All-of-NLP
Repository of all NLP-related things(implement, papers...etc)

NLP와 관련된 아키텍처, 논문 등을 정리한 레포입니다.  
깃허브 마크다운 LaTex 렌더링 이슈로 인헤, 각 설명 문서 당 .pdf와 markdown 두 개의 버전으로 올렸습니다.  
또한, 아래의 링크에서도 확인해보실 수 있습니다.


## 구성
directory 이름을 클릭하시면 해당 directory로 바로 이동할 수 있습니다.  
**각 구성 요소들의 링크는 동일 내용을 업로드한 저의 티스토리 페이지로 이동하는 링크입니다.**  
- [paper](./paper/)  

    각종 논문들을 읽고 리뷰한 내용들을 모아놓은 directory입니다.
    - Adam: A Method for Stochastic Optimization (Only Implement)
    - [Attention is all you need](https://gbdai.tistory.com/46)
    - [BERT: Pre training of Deep Bidirectional Transformers for Language Understanding](https://gbdai.tistory.com/50)
    - [Big Bird: Transformers for Longer Sequences  ](https://gbdai.tistory.com/60)
    - [Dense Passage Retrieval for Open-Domain Question Answering](https://gbdai.tistory.com/72)
    - [Effective Approaches to Attention based Neural Machine Translation](https://gbdai.tistory.com/45)
    - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://gbdai.tistory.com/62)
    - FEVER- a large-scale dataset for Fact Extraction and VERification
    - [Finetuned Language Models Are Zero-Shot Learners](https://gbdai.tistory.com/70)
    - [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://gbdai.tistory.com/51)
    - [Improving Language Understanding by Generative Pre Training](https://gbdai.tistory.com/49)
    - [Language Models are Unsupervised Multitask Learners](https://gbdai.tistory.com/57)
    - [Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering](https://gbdai.tistory.com/68)  
    - [Mixed Precision Training](https://gbdai.tistory.com/40)
    - [Multilingual Language Processing From Bytes](https://gbdai.tistory.com/58)
    - [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://gbdai.tistory.com/71)  
    - [Neural Machine Translation by Jointly Learning to Align and Translate](https://gbdai.tistory.com/44)
    - [REALM: Retrieval-Augmented Language Model Pre-Training](https://gbdai.tistory.com/63)
    - [REPLUG: Retrieval-Augmented Black-Box Language Models](https://gbdai.tistory.com/64)  
    - [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://gbdai.tistory.com/67)  
    - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://gbdai.tistory.com/52)
    - [Self-Attention with Relative Position Representations](https://gbdai.tistory.com/61)
    - [The Natural Language Decathlon- Multitask Learning as Question Answering](https://gbdai.tistory.com/56)
    - [Training language models to follow instructions with human feedback](https://gbdai.tistory.com)  
    - [Using the Output Embedding to Improve Language Models](https://gbdai.tistory.com/48)
  

<br>

- [RNN](./RNN/)  
RNN(Recurrent Neural Network)에 대한 설명을 정리해놓은 곳입니다.  
  - [RNN(Recurrent Neural Network)이란?](https://gbdai.tistory.com/43)   

  
<br>

- [seq2seq](./seq2seq/)  
시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)에 대한 설명과 구현을 정리해놓은 곳입니다.
  - [시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq란? - 기본 구조편)](https://gbdai.tistory.com/37)
  - [Attention이란?-원리부터 masking까지 (General Luong Attention을 기반으로)](https://gbdai.tistory.com/38)

<br>

- [Dataset](./Dataset/)  
Benchmark dataset 및 training dataset 관련 내용을 정리해놓은 곳입니다.
  - [Benchmark](./Dataset/Benchmark/)
  - [Training](./Dataset/Training/)

<br>

## Paper Map
see paper NLP_paper map in [here](https://github.com/augustinLib/NLP-Paper-Map)

