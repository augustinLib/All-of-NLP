# paper
ê°ì¢… ë…¼ë¬¸ë“¤ì„ ì½ê³  ë¦¬ë·°í•œ ë‚´ìš©ë“¤ì„ ëª¨ì•„ë†“ì€ ê³³ì…ë‹ˆë‹¤.


ê¹ƒí—ˆë¸Œ ë§ˆí¬ë‹¤ìš´ LaTex ë Œë”ë§ ì´ìŠˆë¡œ ì¸í—¤, ê° ì„¤ëª… ë¬¸ì„œ ë‹¹ .pdfì™€ .md ë‘ ê°œì˜ ë²„ì „ìœ¼ë¡œ ì˜¬ë ¸ìŠµë‹ˆë‹¤.  
ë˜í•œ, ì•„ë˜ì˜ ë§í¬ì—ì„œë„ í™•ì¸í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  
    
<br>

### ğŸš€TISTORY LINKğŸš€
[Attention is all you need](https://gbdai.tistory.com/46)  

[BERT: Pre training of Deep Bidirectional Transformers for Language Understanding](https://gbdai.tistory.com/50)  

[Effective Approaches to Attention based Neural Machine Translation](https://gbdai.tistory.com/45)  

[GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://gbdai.tistory.com/51)  

[Improving Language Understanding by Generative Pre Training](https://gbdai.tistory.com/49)  

[Language Models are Unsupervised Multitask Learners](https://gbdai.tistory.com/57)

[Mixed Precision Training](https://gbdai.tistory.com/40)  

[Multilingual Language Processing From Bytes](https://gbdai.tistory.com/58)  

[Neural Machine Translation by Jointly Learning to Align and Translate](https://gbdai.tistory.com/44)  

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://gbdai.tistory.com/52)

[The Natural Language Decathlon- Multitask Learning as Question Answering](https://gbdai.tistory.com/56)

[Using the Output Embedding to Improve Language Models](https://gbdai.tistory.com/48)  

